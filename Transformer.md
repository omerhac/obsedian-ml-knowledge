# Transformer architecture
* paper link - http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf

Transformer is a seq2seq model based solely on attention mechanism, without RNNs at all!

[Natural Language Processing](Natural%20Language%20Processing.md)
[Sequence To Sequence](Sequence%20To%20Sequence.md)
[Attention](Attention.md)
