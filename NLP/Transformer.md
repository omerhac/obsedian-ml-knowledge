# Transformer architecture
* paper link - http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
* article link - https://towardsdatascience.com/transformers-explained-65454c0f3fa7

Transformer is a seq2seq model based solely on attention mechanism, without RNNs at all!

[Natural Language Processing](Natural%20Language%20Processing)
[Sequence To Sequence](Sequence%20To%20Sequence)
[Attention](Attention)
